# Day 01 â€“ PySpark Basics in Databricks

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on building strong foundations in Databricks and Apache Spark through hands-on practice.

The objective of Day 1 was to understand the Databricks workspace, set up compute, and get comfortable working with PySpark DataFrames while bridging PySpark and SQL workflows.

---

## Topics Covered
- Overview of SparkSession and its role in Databricks
- Creating PySpark DataFrames from in-memory data
- Understanding DataFrame schema using `printSchema()`
- Basic data exploration using `show()`, `count()`, and `select()`
- Filtering data using conditional expressions
- Performing transformations by adding derived columns
- Aggregating data using `groupBy()` and built-in functions
- Creating temporary views from DataFrames
- Running Spark SQL queries on PySpark DataFrames

---

## Key Learning Outcomes
Through this notebook, I learned how Databricks enables:
- Distributed data processing using PySpark
- Seamless switching between PySpark and SQL on the same dataset
- A unified Lakehouse-style workflow for data engineering and analytics

This hands-on exercise helped solidify core Spark concepts while demonstrating how Databricks simplifies big data processing.

---

## Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Spark SQL

---

ðŸ“Œ This notebook represents **Day 1 progress** in my Databricks learning journey, guided by **Indian Data Club** , **Codebasics** and **Databricks** .
