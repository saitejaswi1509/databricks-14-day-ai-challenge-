{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cefa05bf-40cb-49fb-9afa-94bf40232ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load E-Commerce Transactions Dataset\n",
    "\n",
    "In this step,loading the `default.ecommerce_transactions` table from Databricks into a Spark DataFrame.  \n",
    "This dataset will be used throughout Day 11 for statistical analysis and feature engineering for ML prep.\n",
    "\n",
    "To quick understanding of the structure displaying the first few rows of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9708c840-3197-4d61-a578-9240dbe704db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Transaction_ID</th><th>User_Name</th><th>Age</th><th>Country</th><th>Product_Category</th><th>Purchase_Amount</th><th>Payment_Method</th><th>Transaction_Date</th></tr></thead><tbody><tr><td>1</td><td>Ava Hall</td><td>63</td><td>Mexico</td><td>Clothing</td><td>780.69</td><td>Debit Card</td><td>2023-04-14</td></tr><tr><td>2</td><td>Sophia Hall</td><td>59</td><td>India</td><td>Beauty</td><td>738.56</td><td>PayPal</td><td>2023-07-30</td></tr><tr><td>3</td><td>Elijah Thompson</td><td>26</td><td>France</td><td>Books</td><td>178.34</td><td>Credit Card</td><td>2023-09-17</td></tr><tr><td>4</td><td>Elijah White</td><td>43</td><td>Mexico</td><td>Sports</td><td>401.09</td><td>UPI</td><td>2023-06-21</td></tr><tr><td>5</td><td>Ava Harris</td><td>48</td><td>Germany</td><td>Beauty</td><td>594.83</td><td>Net Banking</td><td>2024-10-29</td></tr><tr><td>6</td><td>Elijah Harris</td><td>51</td><td>India</td><td>Toys</td><td>966.5</td><td>Cash on Delivery</td><td>2025-01-18</td></tr><tr><td>7</td><td>Oliver Clark</td><td>27</td><td>Germany</td><td>Home & Kitchen</td><td>341.73</td><td>Credit Card</td><td>2024-03-13</td></tr><tr><td>8</td><td>Olivia Allen</td><td>46</td><td>Canada</td><td>Home & Kitchen</td><td>11.33</td><td>Debit Card</td><td>2024-01-04</td></tr><tr><td>9</td><td>Liam Harris</td><td>54</td><td>France</td><td>Beauty</td><td>279.43</td><td>Cash on Delivery</td><td>2023-12-06</td></tr><tr><td>10</td><td>Liam Allen</td><td>60</td><td>Canada</td><td>Beauty</td><td>223.9</td><td>Cash on Delivery</td><td>2023-08-07</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ava Hall",
         63,
         "Mexico",
         "Clothing",
         780.69,
         "Debit Card",
         "2023-04-14"
        ],
        [
         2,
         "Sophia Hall",
         59,
         "India",
         "Beauty",
         738.56,
         "PayPal",
         "2023-07-30"
        ],
        [
         3,
         "Elijah Thompson",
         26,
         "France",
         "Books",
         178.34,
         "Credit Card",
         "2023-09-17"
        ],
        [
         4,
         "Elijah White",
         43,
         "Mexico",
         "Sports",
         401.09,
         "UPI",
         "2023-06-21"
        ],
        [
         5,
         "Ava Harris",
         48,
         "Germany",
         "Beauty",
         594.83,
         "Net Banking",
         "2024-10-29"
        ],
        [
         6,
         "Elijah Harris",
         51,
         "India",
         "Toys",
         966.5,
         "Cash on Delivery",
         "2025-01-18"
        ],
        [
         7,
         "Oliver Clark",
         27,
         "Germany",
         "Home & Kitchen",
         341.73,
         "Credit Card",
         "2024-03-13"
        ],
        [
         8,
         "Olivia Allen",
         46,
         "Canada",
         "Home & Kitchen",
         11.33,
         "Debit Card",
         "2024-01-04"
        ],
        [
         9,
         "Liam Harris",
         54,
         "France",
         "Beauty",
         279.43,
         "Cash on Delivery",
         "2023-12-06"
        ],
        [
         10,
         "Liam Allen",
         60,
         "Canada",
         "Beauty",
         223.9,
         "Cash on Delivery",
         "2023-08-07"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Transaction_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "User_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product_Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Purchase_Amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Payment_Method",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\": \"yyyy-M-d\"}",
         "name": "Transaction_Date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.table(\"default.ecommerce_transactions\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5dde1fa-561f-4749-bbe2-bfd66c673518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Descriptive Statistics \n",
    "\n",
    "This step focuses on understanding the overall distribution and quality of the data.\n",
    "\n",
    "calculating:\n",
    "- Basic descriptive statistics (count, mean, std, min, max) for numeric columns like `Purchase_Amount` and `Age`\n",
    "- Total row count and transaction counts\n",
    "- Missing value checks for key columns\n",
    "- Approximate quartiles (Q1 and Q3) for `Purchase_Amount` to understand data spread and detect potential outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0961c9d-2624-46e1-9e4d-f090c7379700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n|summary|   Purchase_Amount|               Age|\n+-------+------------------+------------------+\n|  count|             50000|             50000|\n|   mean|503.15979300000197|          43.96868|\n| stddev|286.56355761465926|15.260577864626729|\n|    min|              5.04|                18|\n|    max|            999.98|                70|\n+-------+------------------+------------------+\n\n+-----+------------+---------------+--------------------+---------------------+----------------+\n|rows |txn_id_count|distinct_txn_id|null_purchase_amount|null_transaction_date|q1_q3_purchase  |\n+-----+------------+---------------+--------------------+---------------------+----------------+\n|50000|50000       |50000          |0                   |0                    |[255.35, 751.16]|\n+-----+------------+---------------+--------------------+---------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# 1) Numeric descriptive stats\n",
    "df.select(\"Purchase_Amount\", \"Age\").describe().show()\n",
    "\n",
    "# 2) Extra summary metrics\n",
    "df.select(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.count(\"Transaction_ID\").alias(\"txn_id_count\"),\n",
    "    F.countDistinct(\"Transaction_ID\").alias(\"distinct_txn_id\"),\n",
    "    F.sum(F.when(F.col(\"Purchase_Amount\").isNull(), 1).otherwise(0)).alias(\"null_purchase_amount\"),\n",
    "    F.sum(F.when(F.col(\"Transaction_Date\").isNull(), 1).otherwise(0)).alias(\"null_transaction_date\"),\n",
    "    F.expr(\"percentile_approx(Purchase_Amount, array(0.25,0.75))\").alias(\"q1_q3_purchase\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a55317d-4f5d-4624-927f-ca9ea984e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Weekday vs Weekend Purchase Analysis\n",
    "\n",
    "In this step, analyzing customer purchasing behavior based on the day of the week.\n",
    "\n",
    "- Deriving a `day_of_week` column from `Transaction_Date`\n",
    "- Create a binary `is_weekend` flag (Saturday & Sunday)\n",
    "- Compare weekday vs weekend transactions using:\n",
    "  - Number of transactions\n",
    "  - Average purchase amount\n",
    "  - Total revenue\n",
    "  - Purchase amount variability (standard deviation)\n",
    "\n",
    "This analysis provides a simple check to see whether weekends show different purchasing patterns compared to weekdays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117e7a98-57dc-4d97-950b-462519464672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+------------+\n|is_weekend|transactions|avg_purchase|total_revenue|std_purchase|\n+----------+------------+------------+-------------+------------+\n|     false|       35616|       501.6| 1.78651117E7|      286.88|\n|      true|       14384|      507.01|   7292877.95|      285.76|\n+----------+------------+------------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.table(\"default.ecommerce_transactions\")\n",
    "\n",
    "df_week = (df\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"Transaction_Date\"))     # 1=Sun, 7=Sat\n",
    "    .withColumn(\"is_weekend\", F.col(\"day_of_week\").isin([1, 7]))\n",
    ")\n",
    "\n",
    "# Compare weekend vs weekday summary\n",
    "(df_week.groupBy(\"is_weekend\")\n",
    " .agg(\n",
    "     F.count(\"*\").alias(\"transactions\"),\n",
    "     F.round(F.avg(\"Purchase_Amount\"), 2).alias(\"avg_purchase\"),\n",
    "     F.round(F.sum(\"Purchase_Amount\"), 2).alias(\"total_revenue\"),\n",
    "     F.round(F.stddev_samp(\"Purchase_Amount\"), 2).alias(\"std_purchase\")\n",
    " )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c092f32-81ec-428e-8bf2-1c120c00c346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identify correlations\n",
    "Purchase_Amount vs Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3822b904-08bb-4d98-9753-4618af1a269d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.003585451717015732"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlations \n",
    "df2.select(\"Purchase_Amount\", \"Age\", \"day_of_week\").na.drop().stat.corr(\"Purchase_Amount\", \"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9116b4e3-9d59-42d6-a8f7-a00a6c6955c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering for Machine Learning Preparation\n",
    "In this step, creating simple meaningful features to make the dataset more suitable for machine learning models.\n",
    "\n",
    "Engineered features include:\n",
    "- `day_of_week`: captures weekly purchasing patterns\n",
    "- `purchase_log`: log transformation of `Purchase_Amount` to reduce skewness\n",
    "- `days_since_last_purchase`: measures customer purchase recency using a window function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b607234-7236-41af-b28d-66b5b1020f6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Transaction_ID</th><th>User_Name</th><th>Purchase_Amount</th><th>day_of_week</th><th>purchase_log</th><th>days_since_last_purchase</th></tr></thead><tbody><tr><td>31528</td><td>Ava Allen</td><td>86.13</td><td>6</td><td>4.467401256243195</td><td>null</td></tr><tr><td>41902</td><td>Ava Allen</td><td>496.31</td><td>6</td><td>6.209213574104885</td><td>0</td></tr><tr><td>33019</td><td>Ava Allen</td><td>821.93</td><td>2</td><td>6.712871142381708</td><td>3</td></tr><tr><td>9819</td><td>Ava Allen</td><td>532.07</td><td>3</td><td>6.2786527476250935</td><td>1</td></tr><tr><td>27055</td><td>Ava Allen</td><td>839.22</td><td>3</td><td>6.733663762308199</td><td>0</td></tr><tr><td>49765</td><td>Ava Allen</td><td>325.6</td><td>3</td><td>5.788736180536365</td><td>0</td></tr><tr><td>14314</td><td>Ava Allen</td><td>762.08</td><td>5</td><td>6.637362875067317</td><td>2</td></tr><tr><td>47599</td><td>Ava Allen</td><td>748.44</td><td>6</td><td>6.619326260969298</td><td>1</td></tr><tr><td>42370</td><td>Ava Allen</td><td>800.69</td><td>4</td><td>6.686721999477285</td><td>5</td></tr><tr><td>18967</td><td>Ava Allen</td><td>718.12</td><td>7</td><td>6.578028242265144</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         31528,
         "Ava Allen",
         86.13,
         6,
         4.467401256243195,
         null
        ],
        [
         41902,
         "Ava Allen",
         496.31,
         6,
         6.209213574104885,
         0
        ],
        [
         33019,
         "Ava Allen",
         821.93,
         2,
         6.712871142381708,
         3
        ],
        [
         9819,
         "Ava Allen",
         532.07,
         3,
         6.2786527476250935,
         1
        ],
        [
         27055,
         "Ava Allen",
         839.22,
         3,
         6.733663762308199,
         0
        ],
        [
         49765,
         "Ava Allen",
         325.6,
         3,
         5.788736180536365,
         0
        ],
        [
         14314,
         "Ava Allen",
         762.08,
         5,
         6.637362875067317,
         2
        ],
        [
         47599,
         "Ava Allen",
         748.44,
         6,
         6.619326260969298,
         1
        ],
        [
         42370,
         "Ava Allen",
         800.69,
         4,
         6.686721999477285,
         5
        ],
        [
         18967,
         "Ava Allen",
         718.12,
         7,
         6.578028242265144,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Transaction_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "User_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Purchase_Amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "purchase_log",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "days_since_last_purchase",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "df = spark.table(\"default.ecommerce_transactions\")\n",
    "# Window per user ordered by date\n",
    "w = Window.partitionBy(\"User_Name\").orderBy(\"Transaction_Date\")\n",
    "df_feat = (df\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"Transaction_Date\"))\n",
    "    .withColumn(\"purchase_log\", F.log(F.col(\"Purchase_Amount\") + F.lit(1)))\n",
    "    .withColumn(\"prev_txn_date\", F.lag(\"Transaction_Date\").over(w))\n",
    "    .withColumn(\"days_since_last_purchase\",\n",
    "                F.datediff(F.col(\"Transaction_Date\"), F.col(\"prev_txn_date\")))\n",
    ")\n",
    "display(df_feat.select(\n",
    "    \"Transaction_ID\",\"User_Name\",\"Purchase_Amount\",\n",
    "    \"day_of_week\",\"purchase_log\",\"days_since_last_purchase\"\n",
    ").limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day11_stat_analysis_MLprep",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}