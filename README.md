# ğŸš€ Databricks 14-Day AI Challenge

This repository documents my complete learning journey through the **Databricks 14-Day AI Challenge**, organized by **Codebasics** and **Indian Data Club** in collaboration with **Databricks**.

Over 14 days, I built an **end-to-end Lakehouse + Analytics + Machine Learning workflow** using Databricks, PySpark, Delta Lake, SQL, MLflow, and Spark ML â€” with consistent hands-on implementation and real-world use cases.

---

## ğŸ“Œ Challenge Objectives
- Build strong foundations in Databricks & Apache Spark
- Design scalable data engineering pipelines
- Apply Delta Lake for reliability and performance
- Perform SQL analytics & dashboarding
- Prepare data for Machine Learning
- Train, track, and compare ML models using MLflow

---

## ğŸ—‚ï¸ Repository Structure

Each folder represents one day of learning, including notebooks, markdown explanations, and hands-on outputs

---

## ğŸ“˜ Day-wise Learning Summary

### **Day 1 â€“ PySpark & Databricks Basics**
- Databricks overview and Lakehouse fundamentals
- PySpark DataFrames and Spark SQL
- Basic transformations and aggregations  
**Hands-on:** Created first PySpark notebook and explored datasets

---

### **Day 2 â€“ Apache Spark Fundamentals**
- Spark architecture (Driver, Executors)
- DataFrames vs RDDs
- Joins and aggregations at scale  
**Hands-on:** E-commerce dataset exploration using Spark SQL

---

### **Day 3 â€“ PySpark Transformations Deep Dive**
- Joins (inner, left, right)
- Window functions (ranking, running totals)
- UDFs for custom logic  
**Hands-on:** Customer segmentation and analytical queries

---

### **Day 4 â€“ Delta Lake Introduction**
- What Delta Lake is and why itâ€™s needed
- ACID transactions for data lakes
- Schema enforcement and validation  
**Hands-on:** Converted CSV to Delta tables and tested schema enforcement

---

### **Day 5 â€“ Delta Lake Advanced Concepts**
- MERGE (upserts)
- Time Travel (versioning)
- OPTIMIZE, ZORDER, VACUUM  
**Hands-on:** Incremental updates and version rollback

---

### **Day 6 â€“ Medallion Architecture (Bronze â†’ Silver â†’ Gold)**
- Purpose of each layer
- Data quality and validation strategies
- Incremental processing patterns  
**Hands-on:** Built full Bronzeâ€“Silverâ€“Gold pipeline

---

### **Day 7 â€“ Workflows & Job Orchestration**
- Multi-task Databricks jobs
- Notebook dependencies
- Scheduling and backfills  
**Hands-on:** Automated end-to-end pipeline execution

---

### **Day 8 â€“ Unity Catalog & Governance**
- Catalogs, schemas, and tables
- Managed vs external tables
- Permissions and access control  
**Hands-on:** Implemented governance and role-based access

---

### **Day 9 â€“ SQL Analytics & Dashboards**
- SQL Warehouses
- Complex analytical queries
- Business KPIs and dashboards  
**Hands-on:** Revenue trends, customer tiers, and funnel analysis

---

### **Day 10 â€“ Performance Optimization**
- Query execution plans
- Partitioning strategies
- OPTIMIZE & ZORDER
- Caching techniques  
**Hands-on:** Benchmarked query performance before and after optimization

---

### **Day 11 â€“ Statistical Analysis & ML Preparation**
- Descriptive statistics
- Hypothesis testing
- Correlation analysis
- Feature engineering basics  
**Hands-on:** Prepared ML-ready dataset

---

### **Day 12 â€“ MLflow Basics**
- MLflow tracking and experiments
- Logging parameters, metrics, and models
- MLflow UI  
**Hands-on:** Tracked regression model experiments

---

### **Day 13 â€“ Model Training & Comparison**
- Spark ML Pipelines
- Training multiple models
- Hyperparameter tuning
- Feature importance  
**Hands-on:** Compared multiple ML models using MLflow

---

### **Day 14 â€“ AI Genie & Mosaic AI (Final Integration)**
- End-to-end ML workflow
- Model evaluation and selection
- Production-style ML thinking  
**Outcome:** Complete Lakehouse + ML pipeline ready for real-world use

---

## ğŸ§  Key Takeaways
- Built industry-style **data engineering pipelines**
- Applied **Delta Lake** for reliability and performance
- Performed **SQL analytics & dashboarding**
- Prepared data for **machine learning**
- Tracked and compared models using **MLflow**
- Developed strong **end-to-end data & ML mindset**

---

## ğŸ™Œ Acknowledgements
Thanks to **Codebasics**, **Indian Data Club**, and **Databricks** for organizing this excellent hands-on learning challenge.

---

## ğŸ“¬ Connect
If you find this repository helpful or want to discuss Databricks, Data Engineering, or Analytics â€” feel free to connect with me on LinkedIn: https://www.linkedin.com/in/saitejaswikondapally/ ğŸš€
