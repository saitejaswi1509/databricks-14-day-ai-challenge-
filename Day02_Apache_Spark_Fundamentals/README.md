# ğŸš€ Day 02 â€“ Apache Spark Fundamentals in Databricks

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on building a strong foundation in **Apache Spark fundamentals** and understanding how Spark operates within the **Databricks platform** through hands-on practice.

The objective of **Day 2** was to understand Sparkâ€™s internal architecture, core abstractions, and execution model, while applying fundamental **PySpark DataFrame operations** on a real-world e-commerce dataset.

---

## ğŸ“š Topics Covered

- âš™ï¸ **Spark Architecture**
  - Driver
  - Executors
  - DAG (Directed Acyclic Graph)
- ğŸ“Š **DataFrames vs RDDs**
- â³ **Lazy Evaluation in Apache Spark**
- ğŸ§© **Databricks Notebook Magic Commands**
  - `%python`
  - `%sql`
  - `%fs`
- ğŸ“¥ Loading data into Spark DataFrames
- ğŸ” Basic DataFrame operations:
  - `select()`
  - `filter()`
  - `groupBy()`
  - `orderBy()`

---

## ğŸ› ï¸ Hands-on Tasks Performed

- ğŸ“ Uploaded a sample **e-commerce transactions CSV dataset** (from Kaggle) into Databricks using **Data Ingestion**
- ğŸ“„ Read the dataset into a Spark DataFrame using **PySpark**
- ğŸ§ª Explored the schema and displayed data in tabular format
- ğŸ”„ Performed core DataFrame transformations and aggregations, including:
  - Selecting specific columns
  - Filtering records based on conditions
  - Grouping and aggregating transactional data
  - Ordering results to derive insights

---

## ğŸ¯ Key Learning Outcomes

Through this notebook, I learned:

- How Sparkâ€™s **driverâ€“executor architecture** enables distributed data processing
- Why **DataFrames are preferred over RDDs** for analytics and data engineering workloads
- How **lazy evaluation** allows Spark to optimize execution plans efficiently
- How Databricks notebooks enable a **seamless PySpark + SQL workflow**
- How to apply fundamental Spark transformations on **real-world datasets**

This exercise strengthened my understanding of **Spark internals** while reinforcing how **Databricks simplifies large-scale data processing**.

---

## ğŸ§° Tools & Technologies

- ğŸŸ£ Databricks Community Edition  
- ğŸ”¥ Apache Spark (PySpark)  
- ğŸ§® Spark SQL  

---

ğŸ“Œ This notebook represents **Day 2 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
